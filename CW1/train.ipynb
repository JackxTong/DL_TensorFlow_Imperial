{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend after setting: torch\n",
      "TensorFlow GPUs: []\n",
      "PyTorch Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1741046505.006170  724454 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1741046505.007766  724454 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1741046505.048424  724454 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1741046505.265894  724454 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "# Suppress warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "# set backend\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "import keras\n",
    "print(\"Backend after setting:\", keras.config.backend())\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "# Check GPU visibility\n",
    "print(\"TensorFlow GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"PyTorch Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">25,691,136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu (\u001b[38;5;33mReLU\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu_1 (\u001b[38;5;33mReLU\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu_2 (\u001b[38;5;33mReLU\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │    \u001b[38;5;34m25,691,136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │         \u001b[38;5;34m4,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu_3 (\u001b[38;5;33mReLU\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m131,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu_4 (\u001b[38;5;33mReLU\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,937,600</span> (98.94 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,937,600\u001b[0m (98.94 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,934,848</span> (98.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,934,848\u001b[0m (98.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,752</span> (10.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,752\u001b[0m (10.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from keras.layers import (Input, Conv2D, BatchNormalization, ReLU, MaxPooling2D, \n",
    "                          Flatten, Dense, Dropout, Lambda)\n",
    "from keras.initializers import HeNormal\n",
    "import keras.ops as K\n",
    "\n",
    "def get_model(hidden_units, output_units, input_shape, rate, l2_coeff=1e-5):\n",
    "    \"\"\"\n",
    "    Creates a face verification model that outputs normalized embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential([Input(shape=input_shape)])\n",
    "\n",
    "    # --- Convolutional blocks / Feature extraction backbone ---\n",
    "\n",
    "    # note we use he kaiming initialization for the weights\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', kernel_initializer=HeNormal(),\n",
    "                     kernel_regularizer=regularizers.l2(l2_coeff)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    # 2nd block\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer=HeNormal(),\n",
    "                     kernel_regularizer=regularizers.l2(l2_coeff)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    # 3rd block\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer=HeNormal(),\n",
    "                     kernel_regularizer=regularizers.l2(l2_coeff)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # --- Fully connected layers ---\n",
    "    for units in hidden_units:\n",
    "        model.add(Dense(units, kernel_initializer=HeNormal(),\n",
    "                        kernel_regularizer=regularizers.l2(l2_coeff)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(ReLU())\n",
    "        model.add(Dropout(rate))\n",
    "\n",
    "    # --- Output layer + normalization ---\n",
    "    model.add(Dense(output_units, kernel_initializer=HeNormal()))\n",
    "    model.add(Lambda(lambda x: x / K.norm(x, axis=1, keepdims=True)))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model(\n",
    "    hidden_units=[1024, 128],\n",
    "    output_units=128,\n",
    "    input_shape=(112, 112, 3),\n",
    "    rate=0.5\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# from tensorflow.keras.utils import Sequence\n",
    "# from PIL import Image\n",
    "\n",
    "# class KerasFaceDataset(Sequence):\n",
    "#     \"\"\"\n",
    "#     Keras-compatible dataset that:\n",
    "#       - Takes a dataset path and extracts image samples with labels\n",
    "#       - Selects up to max_images_per_identity from each identity\n",
    "#       - Allows dataset splitting using indexes (train/validation sets)\n",
    "#       - Returns batches of (images, labels) in (N, 112, 112, 3) format\n",
    "#     \"\"\"\n",
    "#     def __init__(self, dataset_path, identities, max_images_per_identity=10,\n",
    "#                  batch_size=1024, shuffle=True, sample_indexes=None):\n",
    "#         self.dataset_path = dataset_path\n",
    "#         self.identities = identities\n",
    "#         self.max_images_per_identity = max_images_per_identity\n",
    "#         self.batch_size = batch_size\n",
    "#         self.shuffle = shuffle\n",
    "        \n",
    "#         self.image_paths = []\n",
    "#         self.labels = []\n",
    "        \n",
    "#         # Collect image paths and their identity labels\n",
    "#         for idx, identity in enumerate(identities):\n",
    "#             identity_folder = os.path.join(dataset_path, identity)\n",
    "#             image_files = sorted(os.listdir(identity_folder))  # Ensure consistency\n",
    "#             selected_images = image_files[:max_images_per_identity]\n",
    "#             for img_name in selected_images:\n",
    "#                 self.image_paths.append(os.path.join(identity_folder, img_name))\n",
    "#                 self.labels.append(idx)\n",
    "        \n",
    "#         self.indexes = np.arange(len(self.image_paths))\n",
    "        \n",
    "#         # If sample indexes are provided, filter dataset\n",
    "#         if sample_indexes is not None:\n",
    "#             self.image_paths = [self.image_paths[i] for i in sample_indexes]\n",
    "#             self.labels = [self.labels[i] for i in sample_indexes]\n",
    "#             self.indexes = np.arange(len(self.image_paths))\n",
    "\n",
    "#         self.on_epoch_end()  # Shuffle at the start if needed\n",
    "\n",
    "#     def __len__(self):\n",
    "#         \"\"\"Number of batches per epoch.\"\"\"\n",
    "#         return int(np.ceil(len(self.image_paths) / float(self.batch_size)))\n",
    "\n",
    "#     def on_epoch_end(self):\n",
    "#         \"\"\"Shuffle indexes after each epoch if needed.\"\"\"\n",
    "#         if self.shuffle:\n",
    "#             np.random.shuffle(self.indexes)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         \"\"\"Generates one batch of data.\"\"\"\n",
    "#         batch_indexes = self.indexes[index * self.batch_size:\n",
    "#                                      (index + 1) * self.batch_size]\n",
    "        \n",
    "#         batch_paths = [self.image_paths[i] for i in batch_indexes]\n",
    "#         batch_labels = [self.labels[i] for i in batch_indexes]\n",
    "\n",
    "#         images = []\n",
    "#         for path in batch_paths:\n",
    "#             img = Image.open(path).convert(\"RGB\")\n",
    "#             img = img.resize((112, 112), resample=Image.BILINEAR)\n",
    "#             img = np.array(img, dtype=np.float32) / 255.0  # Scale to [0, 1]\n",
    "#             images.append(img)\n",
    "\n",
    "#         images = np.stack(images, axis=0)  # Convert to (N, 112, 112, 3)\n",
    "#         labels = np.array(batch_labels, dtype=np.int32)\n",
    "        \n",
    "#         return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of identities in dataset: 8000\n",
      "Number of batches in full dataset: 79\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load dataset identities\n",
    "# dataset_path = \"data/casia-webface\"\n",
    "# identities = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "# print(\"Number of identities in dataset:\", len(identities))\n",
    "\n",
    "# # Create a full dataset instance\n",
    "# full_dataset = KerasFaceDataset(dataset_path, identities, max_images_per_identity=10, batch_size=1024, shuffle=True)\n",
    "# print(\"Number of batches in full dataset:\", len(full_dataset))\n",
    "\n",
    "# # Split sample indexes randomly (instead of by identity)\n",
    "# indices = np.arange(len(full_dataset.image_paths))\n",
    "# train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create train and validation datasets\n",
    "# train_dataset = KerasFaceDataset(dataset_path, identities,\n",
    "#                                  max_images_per_identity=10, \n",
    "#                                  batch_size=1024, sample_indexes=train_indices)\n",
    "# val_dataset = KerasFaceDataset(dataset_path, identities, \n",
    "#                                max_images_per_identity=10, batch_size=1024, \n",
    "#                                sample_indexes=val_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total_data_size = ideneitities * max_per_identity = 8000 * 10\n",
    "\n",
    "each batch need 10 faces per identity, so each batch need 8000 pics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 63\n",
      "Validation dataset size: 16\n",
      "Images shape: (1024, 112, 112, 3), Labels shape: (1024,)\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "# print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "# # check shape\n",
    "# for images, labels in train_dataset:\n",
    "#     print(f\"Images shape: {images.shape}, Labels shape: {labels.shape}\")\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "\n",
    "class PytorchFaceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A single unified class that:\n",
    "      - Loads up to `max_images_per_identity` images from each identity folder.\n",
    "      - Optionally uses `sample_indexes` for train/val slicing.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, dataset_path, identities,\n",
    "        max_images_per_identity=10,\n",
    "        sample_indexes=None,\n",
    "        classes_per_batch=None,\n",
    "        samples_per_class=None\n",
    "    ):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.identities = identities\n",
    "        self.max_images_per_identity = max_images_per_identity\n",
    "        self.sample_indexes = sample_indexes\n",
    "        self.classes_per_batch = classes_per_batch\n",
    "        self.samples_per_class = samples_per_class\n",
    "\n",
    "        self.image_paths = []\n",
    "        self.labels = []  # We'll group by label = idx in `identities`\n",
    "        \n",
    "        # Gather all images & labels\n",
    "        for idx, identity in enumerate(identities):\n",
    "            identity_folder = os.path.join(dataset_path, identity)\n",
    "            if not os.path.isdir(identity_folder):\n",
    "                continue  # skip if folder doesn't exist\n",
    "\n",
    "            image_files = sorted(os.listdir(identity_folder))\n",
    "            # Take up to max_images_per_identity images\n",
    "            selected_images = image_files[:max_images_per_identity]\n",
    "            for img_name in selected_images:\n",
    "                self.image_paths.append(os.path.join(identity_folder, img_name))\n",
    "                self.labels.append(idx)\n",
    "\n",
    "        # sample_indexes is train_indices or val_indices\n",
    "        if self.sample_indexes is not None:\n",
    "            self.image_paths = [self.image_paths[i] for i in self.sample_indexes]\n",
    "            self.labels = [self.labels[i] for i in self.sample_indexes]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.image_paths[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        # Load image, convert to RGB, resize to (112, 112)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = img.resize((112, 112), resample=Image.BILINEAR)\n",
    "        # Normalize to [0, 1]\n",
    "        img = np.array(img, dtype=np.float32) / 255.0  # shape: (112, 112, 3)\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)  # currently (112, 112, 3)\n",
    "\n",
    "        return img_tensor, label\n",
    "\n",
    "    def get_dataloader(self, batch_size=128, shuffle=True, num_workers=4):\n",
    "        # If we have custom-batch parameters set, build the custom sampler\n",
    "        if self.classes_per_batch is not None and self.samples_per_class is not None:\n",
    "            return DataLoader(\n",
    "                self,\n",
    "                batch_sampler=self._BatchSampler(\n",
    "                    self.labels,\n",
    "                    self.classes_per_batch,\n",
    "                    self.samples_per_class\n",
    "                ),\n",
    "                num_workers=num_workers\n",
    "            )\n",
    "        else:\n",
    "            return DataLoader(\n",
    "                self,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=shuffle,\n",
    "                num_workers=num_workers\n",
    "            )\n",
    "\n",
    "    class _BatchSampler(Sampler):\n",
    "        \"\"\"\n",
    "        Ensures each batch contains `classes_per_batch` classes, each has\n",
    "        `samples_per_class` samples or fewer.\n",
    "        \"\"\"\n",
    "        def __init__(self, labels, classes_per_batch, samples_per_class):\n",
    "            self.labels = labels\n",
    "            self.classes_per_batch = classes_per_batch\n",
    "            self.samples_per_class = samples_per_class\n",
    "\n",
    "            # Group indices by class\n",
    "            self.class_to_indices = {}\n",
    "            for idx, label in enumerate(labels):\n",
    "                self.class_to_indices.setdefault(label, []).append(idx)\n",
    "\n",
    "            # Keep list of all classes for shuffling each epoch\n",
    "            self.all_classes = list(self.class_to_indices.keys())\n",
    "\n",
    "        def __iter__(self):\n",
    "            '''Defines how the batch indices are generated.'''\n",
    "            random.shuffle(self.all_classes) # Shuffle list of all labels each epoch\n",
    "\n",
    "            # We'll chunk the shuffled class list in groups of 'classes_per_batch'\n",
    "\n",
    "            # loop over shuffled classes in chunks of 'classes_per_batch'\n",
    "            # e.g. 8000 classes, 10 class per epoch, so 800 iterations\n",
    "            for start in range(0, len(self.all_classes), self.classes_per_batch):\n",
    "                chunk_classes = self.all_classes[start:start + self.classes_per_batch]\n",
    "                \n",
    "                batch_indices = []\n",
    "                for cls in chunk_classes: # collect sample indices for each class\n",
    "                    idx_list = self.class_to_indices[cls] # map label to indices\n",
    "\n",
    "                    # if a class has enough samples, sample 'samples_per_class' indices\n",
    "                    # otherwise, take all indices\n",
    "                    if len(idx_list) >= self.samples_per_class:\n",
    "                        chosen = random.sample(idx_list, self.samples_per_class)\n",
    "                    else:\n",
    "                        chosen = idx_list  # class is smaller than desired\n",
    "                    batch_indices.extend(chosen)\n",
    "\n",
    "                yield batch_indices # move to dataloader\n",
    "\n",
    "        def __len__(self):\n",
    "            ''' computes how many batches are needed to cover all classes '''\n",
    "            # e.g. 8000 class, 10 class per batch, so (8010-1) // 10 = 800\n",
    "            return (len(self.all_classes) + self.classes_per_batch - 1) // self.classes_per_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we have 8000 identities, and choose 10 images from each ideneity, so total size = 80k\n",
    "\n",
    "We have 800 batches, and within each batch, 10 classes and $\\leq$ 10 samples from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.9/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/usr/lib64/python3.9/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([83, 112, 112, 3]), Labels shape: torch.Size([83])\n",
      "Unique labels in this batch: 10\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(80000)\n",
    "train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "train_torch_dataset = PytorchFaceDataset(dataset_path, identities, \n",
    "                                         max_images_per_identity=10, \n",
    "                                         sample_indexes=train_indices,\n",
    "                                         classes_per_batch=10,   # each batch will pick 10 classes\n",
    "                                        samples_per_class=10) # each class has 10 samples\n",
    "train_loader = train_torch_dataset.get_dataloader(num_workers=4) # batch size = 10*10=100\n",
    "# val_torch_dataset = PytorchFaceDataset(dataset_path, identities,\n",
    "#                                        max_images_per_identity=10, \n",
    "#                                        sample_indexes=val_indices)\n",
    "val_torch_dataset = PytorchFaceDataset(dataset_path, identities,\n",
    "                                       max_images_per_identity=10, \n",
    "                                       sample_indexes=val_indices,\n",
    "                                       classes_per_batch=10,   # each batch will pick 10 classes\n",
    "                                       samples_per_class=10) # each class has 10 samples\n",
    "# val_loader = val_torch_dataset.get_dataloader(batch_size=128, shuffle=False)\n",
    "val_loader = val_torch_dataset.get_dataloader(num_workers=4) # batch size = 10*10=100\n",
    "\n",
    "# Check shape\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Images shape: {images.shape}, Labels shape: {labels.shape}\")\n",
    "    print(\"Unique labels in this batch:\", len(set(labels.tolist())))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def circle_loss_torch(anchor_embeddings, positive_embeddings, negative_embeddings, m=0.25, gamma=64.0):\n",
    "    # -- Step 1: Compute scaled cosine similarities in [0, 1].\n",
    "    s_p = ((anchor_embeddings * positive_embeddings).sum(dim=1) + 1.0) / 2.0\n",
    "    s_n = ((anchor_embeddings * negative_embeddings).sum(dim=1) + 1.0) / 2.0\n",
    "\n",
    "    alpha_p = 1.0 + m - s_p\n",
    "    alpha_n = s_n + m\n",
    "    delta_p = 1.0 - m\n",
    "    delta_n = m\n",
    "\n",
    "    # We'll compute this in a vectorized way and then average across the batch.\n",
    "    inside_term = gamma * (alpha_n * (s_n - delta_n) - alpha_p * (s_p - delta_p))\n",
    "    losses = (1.0 / gamma) * torch.log1p(torch.exp(inside_term))  # log1p(x) = log(1 + x)\n",
    "    \n",
    "    return losses.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to dynamically generate triplets during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "def get_triplets(model, batch_data, margin=0.2, validation=False):\n",
    "    \"\"\"Selects hard triplets from a batch for triplet loss.\"\"\"\n",
    "    images, labels = batch_data  # Unpack batch\n",
    "    labels = labels.cpu().numpy()  # Convert labels to numpy array\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(images)\n",
    "    \n",
    "    # Compute pairwise distances\n",
    "    batch_size = embeddings.shape[0] # should be 1024\n",
    "    distances = torch.cdist(embeddings, embeddings, p=2)  # Euclidean dist\n",
    "\n",
    "    triplets = []\n",
    "    \n",
    "    for anchor_idx in range(batch_size):\n",
    "        anchor_label = labels[anchor_idx]\n",
    "\n",
    "        # Find positive indices\n",
    "        positive_indices = np.where(labels == anchor_label)[0]\n",
    "        positive_indices = positive_indices[positive_indices != anchor_idx]  # Exclude anchor\n",
    "\n",
    "        # print(len(positive_indices), 'positive indices length')\n",
    "\n",
    "        # Find negative indices\n",
    "        negative_indices = np.where(labels != anchor_label)[0]\n",
    "        \n",
    "        # print(len(negative_indices), 'negative indices length')\n",
    "\n",
    "        if len(positive_indices) == 0 or len(negative_indices) == 0:\n",
    "            continue  # Skip if we can't form a triplet\n",
    "\n",
    "        # Select the hardest positive (farthest within same class)\n",
    "        hardest_positive_idx = max(positive_indices, key=lambda i: distances[anchor_idx, i].item())\n",
    "\n",
    "        # semi-hard negative, need to have bigger distance than hardest positive\n",
    "        semi_hard_negatives = [i for i in negative_indices if distances[anchor_idx, i] > distances[anchor_idx, hardest_positive_idx]]\n",
    "        if len(semi_hard_negatives) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            # neg_inx = random.choice(semi_hard_negatives) # randomly choose one negative\n",
    "            hardest_negative_idx = min(semi_hard_negatives, key=lambda i: distances[anchor_idx, i].item())\n",
    "\n",
    "        # print(\"Positive distance:\", distances[anchor_idx, hardest_positive_idx])\n",
    "        # print(\"hardest Negative distance:\", distances[anchor_idx, hardest_negative_idx])\n",
    "\n",
    "        if validation:\n",
    "            # Don't need margin condition\n",
    "            neg_inx = random.choice(negative_indices)\n",
    "            triplets.append((anchor_idx, hardest_positive_idx, neg_inx))\n",
    "            return triplets\n",
    "        \n",
    "        # For training, need to ensure valid triplet (margin condition)\n",
    "        if distances[anchor_idx, hardest_negative_idx] - distances[anchor_idx, hardest_positive_idx] > margin:\n",
    "            triplets.append((anchor_idx, hardest_positive_idx, hardest_negative_idx))\n",
    "\n",
    "    return triplets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 112, 112, 3]) batch 0 shape\n",
      "torch.Size([75]) batch 1 shape\n",
      "2 length of batch\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    # print(batch[0].shape, print(batch[1].shape), print(len(batch)))\n",
    "    print(batch[0].shape, 'batch 0 shape')\n",
    "    print(batch[1].shape, 'batch 1 shape')\n",
    "    print(len(batch), 'length of batch')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of triplets: 73\n",
      "Sample triplet: (0, 5, 70)\n",
      "torch.Size([112, 112, 3])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    triplets = get_triplets(model, (images.to(device), labels.to(device)), margin=1e-5)\n",
    "    print(f\"Number of triplets: {len(triplets)}\")\n",
    "    print(\"Sample triplet:\", triplets[0])\n",
    "    anchor = triplets[0][0]\n",
    "    print(images[anchor].shape) # anchor image shape\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_train_step(mlp_model, loss_fn, opt, train_batch):\n",
    "    \"\"\"PyTorch training step for triplet loss.\"\"\"\n",
    "    mlp_model.zero_grad()\n",
    "\n",
    "    (anchor, positive, negative) = train_batch\n",
    "\n",
    "    anchor_embed = mlp_model(anchor)  # Shape: (batch_size, embedding_dim)\n",
    "    positive_embed = mlp_model(positive)\n",
    "    negative_embed = mlp_model(negative)\n",
    "\n",
    "    loss = loss_fn(anchor_embed, positive_embed, negative_embed)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    grads = [param.grad for param in mlp_model.parameters()]\n",
    "\n",
    "    return loss.item(), grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_valid_step(mlp_model, val_batch):\n",
    "    \"\"\"PyTorch validation step for triplet loss.\"\"\"\n",
    "    (anchor, positive, negative) = val_batch\n",
    "\n",
    "    anchor_embed = mlp_model(anchor)  # Shape: (batch_size, embedding_dim)\n",
    "    positive_embed = mlp_model(positive)\n",
    "    negative_embed = mlp_model(negative)\n",
    "    \n",
    "    pos_dist = torch.norm(anchor_embed - positive_embed, dim=1)  # Distance A→P\n",
    "    neg_dist = torch.norm(anchor_embed - negative_embed, dim=1)  # Distance A→N\n",
    "    # check what percentage of data has anchor_embed closer to positive_embed than negative_embed\n",
    "\n",
    "\n",
    "    correct = (pos_dist < neg_dist).float()  # 1 if correct, 0 if incorrect\n",
    "    accuracy = correct.mean().item()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "loss_metric = keras.metrics.Mean()\n",
    "accuracy_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "val_accuracy_metric = keras.metrics.Mean() \n",
    "# val_loss_metric = keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_custom(mlp_model, loss_fn, opt, training_dataset, validation_dataset, train_step_fn, valid_step_fn, epochs):\n",
    "    \"\"\"\n",
    "    Train the model and evaluate on a validation set.\n",
    "    Returns lists of training and validation losses/accuracies.\n",
    "    \"\"\"\n",
    "    # check if training is using GPU\n",
    "    print(\"Training on GPU:\", next(mlp_model.parameters()).is_cuda)\n",
    "\n",
    "    epoch_losses = []\n",
    "    epoch_acc = []\n",
    "    val_epoch_losses = []\n",
    "    val_epoch_acc = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        # Reset metrics for new epoch\n",
    "        loss_metric.reset_state()\n",
    "        val_accuracy_metric.reset_state()\n",
    "        \n",
    "        # Training loop\n",
    "        for images, labels in training_dataset:\n",
    "            triplets = get_triplets(model, (images.to(device), labels.to(device)), margin=1e-5) # triplet indices\n",
    "            anchors = [triplet[0] for triplet in triplets] # anchor indices\n",
    "            positives = [triplet[1] for triplet in triplets] # positive indices\n",
    "            negatives = [triplet[2] for triplet in triplets] # negative indices\n",
    "\n",
    "            triplet_data = (images[anchors], images[positives], images[negatives])\n",
    "            loss, grads = train_step_fn(mlp_model, loss_fn, opt, train_batch=triplet_data)\n",
    "            opt.apply_gradients(zip(grads, mlp_model.trainable_variables))\n",
    "            \n",
    "            loss_metric.update_state(loss)\n",
    "\n",
    "        # Compute training loss and accuracy\n",
    "        avg_epoch_loss = float(loss_metric.result().cpu().numpy())\n",
    "\n",
    "        # Validation loop\n",
    "        with torch.no_grad():  # Disable gradients for validation\n",
    "            for images, labels in validation_dataset:\n",
    "                triplets = get_triplets(model, (images.to(device), labels.to(device)), margin=1e-5, validation=True)\n",
    "                anchors = [triplet[0] for triplet in triplets]\n",
    "                positives = [triplet[1] for triplet in triplets]\n",
    "                negatives = [triplet[2] for triplet in triplets]\n",
    "                triplet_data = (images[anchors], images[positives], images[negatives])\n",
    "                acc = valid_step_fn(mlp_model, val_batch=triplet_data)\n",
    "                val_accuracy_metric.update_state(acc)\n",
    "\n",
    "        # Compute validation loss and accuracy\n",
    "        avg_val_acc = float(val_accuracy_metric.result().cpu().numpy())\n",
    "\n",
    "        # Store epoch results\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "        # val_epoch_losses.append(avg_val_loss)\n",
    "        val_epoch_acc.append(avg_val_acc)\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch}: loss - {avg_epoch_loss:.4f}, \")\n",
    "\n",
    "        # for param in model.parameters():\n",
    "        #     print('grad:', param.grad)\n",
    "\n",
    "        # save checkpoint\n",
    "        # if epoch % 10 == 0:\n",
    "        print(f\"val_acc - {avg_val_acc:.4f}\")\n",
    "\n",
    "        mlp_model.save(f\"checkpoint_epoch_{epoch}.keras\")\n",
    "\n",
    "\n",
    "        # Save additional metadata separately (like epoch & val accuracy)\n",
    "        metadata = {\n",
    "            'epoch': epoch,\n",
    "            'val_accuracy': avg_val_acc\n",
    "        }\n",
    "        import json\n",
    "        with open(f\"checkpoint_metadata_epoch_{epoch}.json\", \"w\") as f:\n",
    "            json.dump(metadata, f)\n",
    "\n",
    "\n",
    "    return epoch_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU: True\n",
      "Epoch 0/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jx1421/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['sequential/conv2d/kernel', 'sequential/conv2d/bias', 'sequential/conv2d_1/kernel', 'sequential/conv2d_1/bias', 'sequential/conv2d_2/kernel', 'sequential/conv2d_2/bias', 'sequential/dense/kernel', 'sequential/dense/bias', 'sequential/dense_1/kernel', 'sequential/dense_1/bias', 'sequential/dense_2/kernel', 'sequential/dense_2/bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss - 0.3992, \n",
      "val_acc - nan\n",
      "Epoch 1/100\n",
      "Epoch 1: loss - 0.3828, \n",
      "val_acc - 0.5014\n",
      "Epoch 2/100\n",
      "Epoch 2: loss - 0.3997, \n",
      "val_acc - 0.4651\n",
      "Epoch 3/100\n",
      "Epoch 3: loss - 0.3922, \n",
      "val_acc - 0.4986\n",
      "Epoch 4/100\n",
      "Epoch 4: loss - 0.3890, \n",
      "val_acc - 0.5363\n",
      "Epoch 5/100\n",
      "Epoch 5: loss - 0.3780, \n",
      "val_acc - 0.4707\n",
      "Epoch 6/100\n",
      "Epoch 6: loss - 0.3788, \n",
      "val_acc - 0.5265\n",
      "Epoch 7/100\n",
      "Epoch 7: loss - 0.3774, \n",
      "val_acc - 0.4609\n",
      "Epoch 8/100\n",
      "Epoch 8: loss - 0.3766, \n",
      "val_acc - 0.4902\n",
      "Epoch 9/100\n",
      "Epoch 9: loss - 0.3793, \n",
      "val_acc - 0.4986\n",
      "Epoch 10/100\n",
      "Epoch 10: loss - 0.3772, \n",
      "val_acc - 0.5000\n",
      "Epoch 11/100\n",
      "Epoch 11: loss - 0.3767, \n",
      "val_acc - 0.4972\n",
      "Epoch 12/100\n",
      "Epoch 12: loss - 0.3767, \n",
      "val_acc - 0.5000\n",
      "Epoch 13/100\n",
      "Epoch 13: loss - 0.3789, \n",
      "val_acc - 0.5182\n",
      "Epoch 14/100\n",
      "Epoch 14: loss - 0.3769, \n",
      "val_acc - 0.4916\n",
      "Epoch 15/100\n",
      "Epoch 15: loss - 0.3770, \n",
      "val_acc - 0.5056\n",
      "Epoch 16/100\n",
      "Epoch 16: loss - 0.3771, \n",
      "val_acc - 0.5098\n",
      "Epoch 17/100\n",
      "Epoch 17: loss - 0.3774, \n",
      "val_acc - 0.5098\n",
      "Epoch 18/100\n",
      "Epoch 18: loss - 0.3846, \n",
      "val_acc - 0.5265\n",
      "Epoch 19/100\n",
      "Epoch 19: loss - 0.3773, \n",
      "val_acc - 0.4958\n",
      "Epoch 20/100\n",
      "Epoch 20: loss - 0.3774, \n",
      "val_acc - 0.5168\n",
      "Epoch 21/100\n",
      "Epoch 21: loss - 0.3776, \n",
      "val_acc - 0.5279\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0,1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mSGD(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)  \u001b[38;5;66;03m# Try 0.1 or higher\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m epoch_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_custom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcircle_loss_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtraining_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtrain_step_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpt_train_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mvalid_step_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpt_valid_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 22\u001b[0m, in \u001b[0;36mtrain_model_custom\u001b[0;34m(mlp_model, loss_fn, opt, training_dataset, validation_dataset, train_step_fn, valid_step_fn, epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m training_dataset:\n\u001b[0;32m---> 22\u001b[0m     triplets \u001b[38;5;241m=\u001b[39m \u001b[43mget_triplets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmargin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# triplet indices\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     anchors \u001b[38;5;241m=\u001b[39m [triplet[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m triplet \u001b[38;5;129;01min\u001b[39;00m triplets] \u001b[38;5;66;03m# anchor indices\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     positives \u001b[38;5;241m=\u001b[39m [triplet[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m triplet \u001b[38;5;129;01min\u001b[39;00m triplets] \u001b[38;5;66;03m# positive indices\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m, in \u001b[0;36mget_triplets\u001b[0;34m(model, batch_data, margin, validation)\u001b[0m\n\u001b[1;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Convert labels to numpy array\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 11\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Compute pairwise distances\u001b[39;00m\n\u001b[1;32m     14\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# should be 1024\u001b[39;00m\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/keras/src/layers/layer.py:908\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 908\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[1;32m    912\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/keras/src/backend/torch/layer.py:44\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/keras/src/ops/operation.py:46\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m             call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m     42\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[1;32m     43\u001b[0m         call_fn,\n\u001b[1;32m     44\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     45\u001b[0m     )\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/keras/src/models/sequential.py:213\u001b[0m, in \u001b[0;36mSequential.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_functional:\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_functional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# Fallback: Just apply the layer sequence.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# This typically happens if `inputs` is a nested struct.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;66;03m# During each iteration, `inputs` are the inputs to `layer`, and\u001b[39;00m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;66;03m# `outputs` are the outputs of `layer` applied to `inputs`. At the\u001b[39;00m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;66;03m# end of each iteration `inputs` is set to `outputs` to prepare for\u001b[39;00m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;66;03m# the next layer.\u001b[39;00m\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/keras/src/models/functional.py:182\u001b[0m, in \u001b[0;36mFunctional.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m             backend\u001b[38;5;241m.\u001b[39mset_keras_mask(x, mask)\n\u001b[0;32m--> 182\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_through_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unpack_singleton(outputs)\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/keras/src/ops/function.py:171\u001b[0m, in \u001b[0;36mFunction._run_through_graph\u001b[0;34m(self, inputs, operation_fn, call_fn)\u001b[0m\n\u001b[1;32m    169\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m call_fn(op, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node\u001b[38;5;241m.\u001b[39moutputs, tree\u001b[38;5;241m.\u001b[39mflatten(outputs)):\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/keras/src/models/functional.py:637\u001b[0m, in \u001b[0;36moperation_fn.<locals>.call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(operation, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_call_has_training_arg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m operation\u001b[38;5;241m.\u001b[39m_call_has_training_arg\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    635\u001b[0m ):\n\u001b[1;32m    636\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m training\n\u001b[0;32m--> 637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moperation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/keras/src/layers/layer.py:908\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 908\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[1;32m    912\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/keras/src/backend/torch/layer.py:44\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/keras/src/ops/operation.py:46\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m             call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m     42\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[1;32m     43\u001b[0m         call_fn,\n\u001b[1;32m     44\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     45\u001b[0m     )\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/keras/src/layers/core/dense.py:144\u001b[0m, in \u001b[0;36mDense.call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 144\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m         x \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39madd(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/keras/src/ops/numpy.py:3815\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x1, x2)):\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Matmul()\u001b[38;5;241m.\u001b[39msymbolic_call(x1, x2)\n\u001b[0;32m-> 3815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DL_TensorFlow_Imperial/DL_venv/lib64/python3.9/site-packages/keras/src/backend/torch/numpy.py:114\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    112\u001b[0m x1 \u001b[38;5;241m=\u001b[39m cast(x1, compute_dtype)\n\u001b[1;32m    113\u001b[0m x2 \u001b[38;5;241m=\u001b[39m cast(x2, compute_dtype)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m, result_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizer = keras.optimizers.SGD()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.1)  # Try 0.1 or higher\n",
    "\n",
    "epoch_losses = train_model_custom(model, loss_fn=circle_loss_torch, opt=optimizer,\n",
    "                                            training_dataset=train_loader, \n",
    "                                            validation_dataset=val_loader, \n",
    "                                            train_step_fn=pt_train_step, \n",
    "                                            valid_step_fn=pt_valid_step,\n",
    "                                            epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_file = \"./data/lfw_test_pairs.txt\"\n",
    "lfw_folder = \"./data/labeled-faces-in-the-wild\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 4\n",
      "Device 0: NVIDIA RTX A6000\n",
      "Device 0: 47.43GB\n",
      "Device 1: NVIDIA RTX A6000\n",
      "Device 1: 47.43GB\n",
      "Device 2: NVIDIA RTX A6000\n",
      "Device 2: 47.43GB\n",
      "Device 3: NVIDIA RTX A6000\n",
      "Device 3: 47.43GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    num_devices  = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs: {num_devices}\")\n",
    "    for i in range(num_devices):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        total_mem = torch.cuda.get_device_properties(device).total_memory\n",
    "        print(f\"Device {i}: {total_mem/1024**3:.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
